{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcfe6df9",
   "metadata": {
    "id": "bcfe6df9"
   },
   "source": [
    "# Training Llama-3.1 8B with Megatron-LM\n",
    "\n",
    "This tutorial demonstrates how to train the Llama-3.1 model using *mock data*. The Llama-3.1 8B model is a popular open-source large language model (LLM) designed to handle a wide range of natural language processing tasks efficiently. Learn more about the Llama models at [Llama's website](https://www.llama.com/).\n",
    "\n",
    "This tutorial uses mock data to provide a quick and lightweight demonstration of the training workflow, enabling you to verify that your environment is correctly configured and functional. Mock data is a useful way to validate the training pipeline without requiring large datasets.\n",
    "\n",
    "The training process leverages the Megatron-LM framework, a specialized framework for pretraining and fine-tuning large-scale language models. For more information about Megatron-LM, see their [GitHub repository](https://github.com/NVIDIA/Megatron-LM). All steps are executed within a Docker container, which provides a ready-to-use environment with all necessary dependencies.\n",
    "\n",
    "This tutorial builds on the setup completed in the [Pretraining with Megatron-LM tutorial](https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/pretrain/setup_tutorial.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WDQ_-T85Kl7m",
   "metadata": {
    "id": "WDQ_-T85Kl7m"
   },
   "source": [
    "## Prerequisites\n",
    "\n",
    "### Hugging Face API access\n",
    "\n",
    "* Obtain an API token from [Hugging Face](https://huggingface.co) for downloading models.\n",
    "* Ensure the Hugging Face API token has the necessary permissions and approval to access [Meta's Llama checkpoints](https://huggingface.co/meta-llama/Llama-3.1-8B).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zl7fkZ9uKl7n",
   "metadata": {
    "id": "zl7fkZ9uKl7n"
   },
   "source": [
    "## Prepare the training environment\n",
    "\n",
    "After your system meets the prerequisites, follow these steps to set up the training environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9601ad38",
   "metadata": {
    "id": "9601ad38"
   },
   "source": [
    "### 1. Clone the Megatron-LM repository\n",
    "\n",
    "Run the following commands inside the Docker container to clone the Megatron-LM repository and navigate to the validated commit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88bac989",
   "metadata": {
    "id": "88bac989",
    "tags": [
     "docker"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'Megatron-LM' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "# Clone the Megatron-LM repository and navigate to the validated commit\n",
    "!git clone https://github.com/ROCm/Megatron-LM && cd Megatron-LM && git checkout bb93ccbfeae6363c67b361a97a27c74ab86e7e92"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ede6ae0-bf25-49f1-84ba-a34848c7fc87",
   "metadata": {
    "id": "9601ad38"
   },
   "source": [
    "### 2. Complete necessary installs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68f195f4-0feb-4051-9f95-c71c4a9708bb",
   "metadata": {
    "id": "68f195f4-0feb-4051-9f95-c71c4a9708bb",
    "tags": [
     "docker"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface_hub is already installed.\n",
      "regex is already installed.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import huggingface_hub\n",
    "    print(\"huggingface_hub is already installed.\")\n",
    "except ImportError:\n",
    "    !pip install huggingface_hub\n",
    "\n",
    "try:\n",
    "    import regex\n",
    "    print(\"regex is already installed.\")\n",
    "except ImportError:\n",
    "    !pip install regex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e506c4",
   "metadata": {
    "id": "18e506c4"
   },
   "source": [
    "### 3. Provide your Hugging Face token\n",
    "\n",
    "A Hugging Face token can be generated by signing into your account at [Hugging Face Tokens](https://huggingface.co/settings/tokens).\n",
    "\n",
    "You'll require a Hugging Face API token to access Llama-3.1 8B. Generate your token at Hugging Face Tokens and request access for Llama-3.1 8B. Tokens typically start with \"hf_\".\n",
    "\n",
    "Run the following interactive block in your Jupyter notebook to set up the token:\n",
    "\n",
    "**Note**: Uncheck the \"Add token as Git credential\" option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4d549b7",
   "metadata": {
    "id": "b4d549b7"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1f70832c4374ba2944fdb3e03d6ae7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login, HfApi\n",
    "\n",
    "# Prompt the user to log in\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297b8452",
   "metadata": {
    "id": "297b8452"
   },
   "source": [
    "Verify that your token was accepted correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87d624e4",
   "metadata": {
    "id": "87d624e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token validated successfully! Logged in as: andyll7772\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    api = HfApi()\n",
    "    user_info = api.whoami()\n",
    "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
    "except Exception as e:\n",
    "    print(f\"Token validation failed. Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d0a5f3",
   "metadata": {
    "id": "d0d0a5f3"
   },
   "source": [
    "## Run the training script\n",
    "\n",
    "This section describes how to run the training script, with an explanation of the key parameters.\n",
    "\n",
    "### Single-node training overview\n",
    "\n",
    "The training process involves running a pre-configured script that initializes and executes the training of the Llama-3.1 model. The script leverages the Megatron-LM framework and mock data to simulate a full training pipeline. This approach ensures your environment is configured correctly and is functional for real-world use cases.\n",
    "\n",
    "Before running the script, ensure all environment variables are set correctly.\n",
    "\n",
    "### Key parameters for training:\n",
    "\n",
    "* **Batch size (`BS`)**: Set this to `64` for optimal GPU usage.\n",
    "* **Sequence length (`SEQ_LENGTH`)**: Input sequence length, set to `4096`.\n",
    "* **Tensor parallelism (`TP`)**: Set this to `8` for efficient parallelism.\n",
    "* **Precision (`TE_FP8`)**: Set this to `0` for `BF16` precision.\n",
    "\n",
    "### Run the training script\n",
    "\n",
    "Use the following command to train the model on a single node:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fb180bf",
   "metadata": {
    "id": "7fb180bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NO_TRAINING=0\n",
      "Single node setup, skipping NCCL and GLOO socket interface settings.\n",
      "experiment/1nodes_rank0_train_8B_mbs2_bs64_tp8_pp1_cp1_iter10/TE_FP8_0/2025-06-09_22-25-07/output_perf.log\n",
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "/var/lib/jenkins/jupyter/Megatron-LM/megatron/core/tensor_parallel/layers.py:289: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/var/lib/jenkins/jupyter/Megatron-LM/megatron/core/tensor_parallel/layers.py:300: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "/var/lib/jenkins/jupyter/Megatron-LM/megatron/core/tensor_parallel/layers.py:392: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @custom_fwd\n",
      "/var/lib/jenkins/jupyter/Megatron-LM/megatron/core/tensor_parallel/layers.py:432: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  @custom_bwd\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/lib/jenkins/jupyter/Megatron-LM/pretrain_gpt.py\", line 265, in <module>\n",
      "    pretrain(\n",
      "  File \"/var/lib/jenkins/jupyter/Megatron-LM/megatron/training/training.py\", line 245, in pretrain\n",
      "    initialize_megatron(\n",
      "  File \"/var/lib/jenkins/jupyter/Megatron-LM/megatron/training/initialize.py\", line 67, in initialize_megatron\n",
      "    validate_args(args, args_defaults)\n",
      "  File \"/var/lib/jenkins/jupyter/Megatron-LM/megatron/training/arguments.py\", line 181, in validate_args\n",
      "    assert args.world_size % total_model_size == 0, (\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AssertionError: world size (1) is not divisible by total_model_size (encoder_model_size=0 + decoder_model_size=8)\n",
      "E0609 22:25:34.678000 692 site-packages/torch/distributed/elastic/multiprocessing/api.py:869] failed (exitcode: 1) local_rank: 0 (pid: 717) of binary: /opt/conda/envs/py_3.12/bin/python3\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/py_3.12/bin/torchrun\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py\", line 355, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/distributed/run.py\", line 918, in main\n",
      "    run(args)\n",
      "  File \"/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/distributed/run.py\", line 909, in run\n",
      "    elastic_launch(\n",
      "  File \"/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/distributed/launcher/api.py\", line 138, in __call__\n",
      "    return launch_agent(self._config, self._entrypoint, list(args))\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/py_3.12/lib/python3.12/site-packages/torch/distributed/launcher/api.py\", line 269, in launch_agent\n",
      "    raise ChildFailedError(\n",
      "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
      "============================================================\n",
      "pretrain_gpt.py FAILED\n",
      "------------------------------------------------------------\n",
      "Failures:\n",
      "  <NO_OTHER_FAILURES>\n",
      "------------------------------------------------------------\n",
      "Root Cause (first observed failure):\n",
      "[0]:\n",
      "  time      : 2025-06-09_22:25:34\n",
      "  host      : f9fffb81eeaf\n",
      "  rank      : 0 (local_rank: 0)\n",
      "  exitcode  : 1 (pid: 717)\n",
      "  error_file: <N/A>\n",
      "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
      "============================================================\n",
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/redis/connection.py:77: UserWarning: redis-py works best with hiredis. Please consider installing\n",
      "  warnings.warn(msg)\n",
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "throughput per GPU: nan\n",
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/numpy/core/fromnumeric.py:3504: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/opt/conda/envs/py_3.12/lib/python3.12/site-packages/numpy/core/_methods.py:129: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n",
      "elapsed time per iteration: nan\n",
      "tokens/GPU/s: nan\n"
     ]
    }
   ],
   "source": [
    "!cd Megatron-LM && TEE_OUTPUT=1 MBS=2 BS=64 TP=8 TE_FP8=0 SEQ_LENGTH=4096  \\\n",
    "TOKENIZER_MODEL='meta-llama/Llama-3.1-8B' MODEL_SIZE='8' \\\n",
    "bash examples/llama/train_llama3.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c748d2e8",
   "metadata": {
    "id": "c748d2e8"
   },
   "source": [
    "### Additional details about the command\n",
    "\n",
    "This command configures the training process with the following parameters:\n",
    "\n",
    "* **`TEE_OUTPUT=1`**: Enables logging output to the console.\n",
    "* **`MBS=2`**: Micro-batch size per GPU.\n",
    "* **`BS=64`**: Total batch size across all GPUs.\n",
    "* **`TP=8`**: Tensor parallelism for distributing the model across GPUs.\n",
    "* **`TE_FP8=0`**: Sets the precision to `BF16` for training.\n",
    "* **`SEQ_LENGTH=4096`**: Maximum input sequence length.\n",
    "\n",
    "The training script does the following:\n",
    "* Uses mock data as input.\n",
    "* Trains the Llama-3.1 8B model with the specified configurations.\n",
    "\n",
    "You can customize these parameters based on your hardware and desired configurations by modifying the command details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51275b54",
   "metadata": {
    "id": "51275b54"
   },
   "source": [
    "## Monitor the training progress\n",
    "\n",
    "Monitor the output logs during the training process for the following developments:\n",
    "\n",
    "* **Iteration progress**: The number of completed iterations.\n",
    "* **Loss values**: This indicates the model's learning progress. Lower values suggest better learning.\n",
    "* **GPU utilization**: Ensures the optimal usage of your hardware resources.\n",
    "\n",
    "Logs are printed to the console and saved to a log file within the directory specified by the script."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8378cfb",
   "metadata": {
    "id": "a8378cfb"
   },
   "source": [
    "## Key notes\n",
    "\n",
    "* Mock data is for validation only. To use a different dataset, see the [Pretraining with Megatron-LM tutorial](https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/pretrain/setup_tutorial.html).\n",
    "* Tune the hyperparameters based on your hardware. The hyperparameter configuration in this tutorial is based on one node of 8x MI300x GPUs.\n",
    "* This example illustrates how to run a training task on a single node. For multi-node training instructions, see the [Pretraining with Megatron-LM tutorial](https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/pretrain/setup_tutorial.html).\n",
    "* Verify the logs for correctness."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

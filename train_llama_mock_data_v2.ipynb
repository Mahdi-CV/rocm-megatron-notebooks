{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bcfe6df9",
      "metadata": {
        "id": "bcfe6df9"
      },
      "source": [
        "# Training Llama-3.1 8B with Megatron-LM\n",
        "\n",
        "This tutorial demonstrates how to train the Llama-3.1 model using *mock data*. The Llama-3.1 8B model is a popular open-source large language model (LLM) designed to handle a wide range of natural language processing tasks efficiently. Learn more about the Llama models at [Llama's website](https://www.llama.com/).\n",
        "\n",
        "This tutorial uses mock data to provide a quick and lightweight demonstration of the training workflow, enabling you to verify that your environment is correctly configured and functional. Mock data is a useful way to validate the training pipeline without requiring large datasets.\n",
        "\n",
        "The training process leverages the Megatron-LM framework, a specialized framework for pretraining and fine-tuning large-scale language models. For more information about Megatron-LM, see their [GitHub repository](https://github.com/NVIDIA/Megatron-LM). All steps are executed within a Docker container, which provides a ready-to-use environment with all necessary dependencies.\n",
        "\n",
        "This tutorial builds on the setup completed in the [Pretraining with Megatron-LM tutorial](https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/pretrain/setup_tutorial.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WDQ_-T85Kl7m",
      "metadata": {
        "id": "WDQ_-T85Kl7m"
      },
      "source": [
        "## Prerequisites\n",
        "\n",
        "### Hugging Face API access\n",
        "\n",
        "* Obtain an API token from [Hugging Face](https://huggingface.co) for downloading models.\n",
        "* Ensure the Hugging Face API token has the necessary permissions and approval to access [Meta's Llama checkpoints](https://huggingface.co/meta-llama/Llama-3.1-8B).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zl7fkZ9uKl7n",
      "metadata": {
        "id": "zl7fkZ9uKl7n"
      },
      "source": [
        "## Prepare the training environment\n",
        "\n",
        "After your system meets the prerequisites, follow these steps to set up the training environment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9601ad38",
      "metadata": {
        "id": "9601ad38"
      },
      "source": [
        "### 1. Clone the Megatron-LM repository\n",
        "\n",
        "Run the following commands inside the Docker container to clone the Megatron-LM repository and navigate to the validated commit:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88bac989",
      "metadata": {
        "id": "88bac989",
        "tags": [
          "docker"
        ]
      },
      "outputs": [],
      "source": [
        "# Clone the Megatron-LM repository and navigate to the validated commit\n",
        "!git clone https://github.com/ROCm/Megatron-LM && cd Megatron-LM && git checkout bb93ccbfeae6363c67b361a97a27c74ab86e7e92"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18e506c4",
      "metadata": {
        "id": "18e506c4"
      },
      "source": [
        "### 2. Provide your Hugging Face token\n",
        "\n",
        "A Hugging Face token can be generated by signing into your account at [Hugging Face Tokens](https://huggingface.co/settings/tokens).\n",
        "\n",
        "You'll require a Hugging Face API token to access Llama-3.1 8B. Generate your token at Hugging Face Tokens and request access for Llama-3.1 8B. Tokens typically start with \"hf_\".\n",
        "\n",
        "Run the following interactive block in your Jupyter notebook to set up the token:\n",
        "\n",
        "**Note**: Uncheck the \"Add token as Git credential\" option."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4d549b7",
      "metadata": {
        "id": "b4d549b7"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login, HfApi\n",
        "\n",
        "# Prompt the user to log in\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "297b8452",
      "metadata": {
        "id": "297b8452"
      },
      "source": [
        "Verify that your token was accepted correctly:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "87d624e4",
      "metadata": {
        "id": "87d624e4"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    api = HfApi()\n",
        "    user_info = api.whoami()\n",
        "    print(f\"Token validated successfully! Logged in as: {user_info['name']}\")\n",
        "except Exception as e:\n",
        "    print(f\"Token validation failed. Error: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0d0a5f3",
      "metadata": {
        "id": "d0d0a5f3"
      },
      "source": [
        "## Run the training script\n",
        "\n",
        "This section describes how to run the training script, with an explanation of the key parameters.\n",
        "\n",
        "### Single-node training overview\n",
        "\n",
        "The training process involves running a pre-configured script that initializes and executes the training of the Llama-3.1 model. The script leverages the Megatron-LM framework and mock data to simulate a full training pipeline. This approach ensures your environment is configured correctly and is functional for real-world use cases.\n",
        "\n",
        "Before running the script, ensure all environment variables are set correctly.\n",
        "\n",
        "### Key parameters for training:\n",
        "\n",
        "* **Batch size (`BS`)**: Set this to `64` for optimal GPU usage.\n",
        "* **Sequence length (`SEQ_LENGTH`)**: Input sequence length, set to `4096`.\n",
        "* **Tensor parallelism (`TP`)**: Set this to `8` for efficient parallelism.\n",
        "* **Precision (`TE_FP8`)**: Set this to `0` for `BF16` precision.\n",
        "\n",
        "### Run the training script\n",
        "\n",
        "Use the following command to train the model on a single node:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fb180bf",
      "metadata": {
        "id": "7fb180bf"
      },
      "outputs": [],
      "source": [
        "!cd Megatron-LM && TEE_OUTPUT=1 MBS=2 BS=64 TP=8 TE_FP8=0 SEQ_LENGTH=4096  \\\n",
        "TOKENIZER_MODEL='meta-llama/Llama-3.1-8B' MODEL_SIZE='8' \\\n",
        "bash examples/llama/train_llama3.sh"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c748d2e8",
      "metadata": {
        "id": "c748d2e8"
      },
      "source": [
        "### Additional details about the command\n",
        "\n",
        "This command configures the training process with the following parameters:\n",
        "\n",
        "* **`TEE_OUTPUT=1`**: Enables logging output to the console.\n",
        "* **`MBS=2`**: Micro-batch size per GPU.\n",
        "* **`BS=64`**: Total batch size across all GPUs.\n",
        "* **`TP=8`**: Tensor parallelism for distributing the model across GPUs.\n",
        "* **`TE_FP8=0`**: Sets the precision to `BF16` for training.\n",
        "* **`SEQ_LENGTH=4096`**: Maximum input sequence length.\n",
        "\n",
        "The training script does the following:\n",
        "* Uses mock data as input.\n",
        "* Trains the Llama-3.1 8B model with the specified configurations.\n",
        "\n",
        "You can customize these parameters based on your hardware and desired configurations by modifying the command details."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51275b54",
      "metadata": {
        "id": "51275b54"
      },
      "source": [
        "## Monitor the training progress\n",
        "\n",
        "Monitor the output logs during the training process for the following developments:\n",
        "\n",
        "* **Iteration progress**: The number of completed iterations.\n",
        "* **Loss values**: This indicates the model's learning progress. Lower values suggest better learning.\n",
        "* **GPU utilization**: Ensures the optimal usage of your hardware resources.\n",
        "\n",
        "Logs are printed to the console and saved to a log file within the directory specified by the script."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8378cfb",
      "metadata": {
        "id": "a8378cfb"
      },
      "source": [
        "## Key notes\n",
        "\n",
        "* Mock data is for validation only. To use a different dataset, see the [Pretraining with Megatron-LM tutorial](https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/pretrain/setup_tutorial.html).\n",
        "* Tune the hyperparameters based on your hardware. The hyperparameter configuration in this tutorial is based on one node of 8x MI300x GPUs.\n",
        "* This example illustrates how to run a training task on a single node. For multi-node training instructions, see the [Pretraining with Megatron-LM tutorial](https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/pretrain/setup_tutorial.html).\n",
        "* Verify the logs for correctness."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
